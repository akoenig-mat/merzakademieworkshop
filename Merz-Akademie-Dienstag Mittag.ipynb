{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critical Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only the raw text, we'll derive and explore the semantic properties of its words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python code in one module gains access to the code in another module by the process of importing it. The import statement is the most common way of invoking the import machinery, but it is not the only way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import common system-tools etc. here that are not directly connected to NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import sklearn.manifold\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will probably run into an \"ModuleNorFoundError\" here. THis means that the needed module is not installed on your system.\n",
    "You can do that in the anaconda command prompt:\n",
    "for example: <b>\"conda install -c anaconda nltk\"</b> or <b>\"conda install -c anaconda gensim\"</b> and <b>\"conda install -c conda-forge glob2\"</b> <br> for detailed information refer to https://docs.anaconda.com/anaconda/user-guide/tasks/install-packages/ <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Download NLTK tokenizer models (only the first time)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load books from files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_filenames = sorted(glob.glob(\"txtdata\\*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found books:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['txtdata\\\\truth-and-method-gadamer-2004.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Found books:\")\n",
    "book_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine the books into one string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 'txtdata\\truth-and-method-gadamer-2004.txt'...\n",
      "Corpus is now 1618721 characters long\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = u\"\"\n",
    "for book_filename in book_filenames:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build your vocabulary (word tokenization)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into a list of words\n",
    "#rtemove unnnecessary,, split into words, no hyphens\n",
    "#list of words\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence where each word is tokenized\n",
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hermeneutic phenomenon is\n",
      "basically not a problem of method at all.\n",
      "['The', 'hermeneutic', 'phenomenon', 'is', 'basically', 'not', 'a', 'problem', 'of', 'method', 'at', 'all']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentences[5])\n",
    "print(sentence_to_wordlist(raw_sentences[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 260,890 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is a method of computing vector representations of words introduced by a team of researchers at Google led by Tomas Mikolov. Google hosts an open-source version of Word2vec released under an Apache 2.0 license. In 2014, Mikolov left Google for Facebook, and in May 2015, Google was granted a patent for the method, which does not abrogate the Apache license under which it has been released. <br>\n",
    "\n",
    "<b>Foreign Languages</b> <br>\n",
    "\n",
    "While words in all languages may be converted into vectors with Word2vec, and those vectors learned with deep-learning frameworks, NLP preprocessing can be very language specific, and requires tools beyond our libraries. The Stanford Natural Language Processing Group has a number of Java-based tools for tokenization, part-of-speech tagging and named-entity recognition for languages such as Mandarin Chinese, Arabic, French, German and Spanish. For Japanese, NLP tools like Kuromoji are useful. Other foreign-language resources, including text corpora, are available here.\n",
    "http://www-nlp.stanford.edu/links/statnlp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>size (int, optional)</b> – Dimensionality of the word vectors.\n",
    "\n",
    "<b>window (int, optional)</b> – Maximum distance between the current and predicted word within a sentence.\n",
    "\n",
    "<b>min_count (int, optional)</b> – Ignores all words with total frequency lower than this.\n",
    "\n",
    "<b>workers (int, optional)</b> – Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "\n",
    "<b>sg ({0, 1}, optional)</b> – Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "\n",
    "<b>seed (int, optional)</b> – Seed for the random number generator. Initial vectors for each word are seeded with a hash of the concatenation of word + str(seed). Note that for a fully deterministically-reproducible run, you must also limit the model to a single worker thread (workers=1), to eliminate ordering jitter from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires use of the PYTHONHASHSEED environment variable to control hash randomization).\n",
    "\n",
    "<b>sample (float, optional)</b> – The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "\n",
    "<b>iter (int, optional)</b> – Number of iterations (epochs) over the corpus.\n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-26 16:27:52,400 : INFO : collecting all words and their counts\n",
      "2019-11-26 16:27:52,402 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-26 16:27:52,488 : INFO : PROGRESS: at sentence #10000, processed 223470 words, keeping 11480 word types\n",
      "2019-11-26 16:27:52,503 : INFO : collected 12895 word types from a corpus of 260890 raw words and 11459 sentences\n",
      "2019-11-26 16:27:52,507 : INFO : Loading a fresh vocabulary\n",
      "2019-11-26 16:27:52,672 : INFO : min_count=2 retains 7334 unique words (56% of original 12895, drops 5561)\n",
      "2019-11-26 16:27:52,674 : INFO : min_count=2 leaves 255329 word corpus (97% of original 260890, drops 5561)\n",
      "2019-11-26 16:27:52,737 : INFO : deleting the raw counts dictionary of 12895 items\n",
      "2019-11-26 16:27:52,738 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2019-11-26 16:27:52,740 : INFO : downsampling leaves estimated 185415 word corpus (72.6% of prior 255329)\n",
      "2019-11-26 16:27:52,784 : INFO : estimated required memory for 7334 words and 160 dimensions: 13054520 bytes\n",
      "2019-11-26 16:27:52,789 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 7334\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start training, this might take a minute or two...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-26 16:27:53,212 : INFO : training model with 2 workers on 7334 vocabulary and 160 features, using sg=1 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-11-26 16:27:54,283 : INFO : EPOCH 1 - PROGRESS: at 38.26% words, 70544 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:27:55,306 : INFO : EPOCH 1 - PROGRESS: at 72.71% words, 66481 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:27:56,063 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 16:27:56,176 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 16:27:56,178 : INFO : EPOCH - 1 : training on 260890 raw words (185276 effective words) took 2.9s, 63996 effective words/s\n",
      "2019-11-26 16:27:57,295 : INFO : EPOCH 2 - PROGRESS: at 30.60% words, 51920 words/s, in_qsize 4, out_qsize 0\n",
      "2019-11-26 16:27:58,374 : INFO : EPOCH 2 - PROGRESS: at 65.04% words, 55716 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:27:59,275 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 16:27:59,291 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 16:27:59,292 : INFO : EPOCH - 2 : training on 260890 raw words (185484 effective words) took 3.1s, 60136 effective words/s\n",
      "2019-11-26 16:28:00,340 : INFO : EPOCH 3 - PROGRESS: at 34.43% words, 61284 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:28:01,350 : INFO : EPOCH 3 - PROGRESS: at 72.71% words, 65797 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:28:02,022 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 16:28:02,083 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 16:28:02,084 : INFO : EPOCH - 3 : training on 260890 raw words (185379 effective words) took 2.8s, 66650 effective words/s\n",
      "2019-11-26 16:28:03,122 : INFO : EPOCH 4 - PROGRESS: at 34.43% words, 61758 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:28:04,134 : INFO : EPOCH 4 - PROGRESS: at 72.71% words, 65979 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:28:04,807 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 16:28:04,866 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 16:28:04,868 : INFO : EPOCH - 4 : training on 260890 raw words (185184 effective words) took 2.8s, 66821 effective words/s\n",
      "2019-11-26 16:28:05,888 : INFO : EPOCH 5 - PROGRESS: at 34.43% words, 62909 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:28:06,897 : INFO : EPOCH 5 - PROGRESS: at 72.71% words, 66824 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:28:07,609 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 16:28:07,683 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 16:28:07,685 : INFO : EPOCH - 5 : training on 260890 raw words (185527 effective words) took 2.8s, 66146 effective words/s\n",
      "2019-11-26 16:28:08,704 : INFO : EPOCH 6 - PROGRESS: at 34.43% words, 62865 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:28:09,772 : INFO : EPOCH 6 - PROGRESS: at 72.71% words, 64857 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:28:10,438 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 16:28:10,460 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 16:28:10,464 : INFO : EPOCH - 6 : training on 260890 raw words (185572 effective words) took 2.8s, 66975 effective words/s\n",
      "2019-11-26 16:28:11,508 : INFO : EPOCH 7 - PROGRESS: at 34.43% words, 61349 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:28:12,540 : INFO : EPOCH 7 - PROGRESS: at 72.71% words, 65207 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:28:13,213 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 16:28:13,270 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 16:28:13,272 : INFO : EPOCH - 7 : training on 260890 raw words (185545 effective words) took 2.8s, 66294 effective words/s\n",
      "2019-11-26 16:28:14,338 : INFO : EPOCH 8 - PROGRESS: at 34.43% words, 60166 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:28:15,355 : INFO : EPOCH 8 - PROGRESS: at 72.71% words, 64974 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:28:16,029 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 16:28:16,058 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 16:28:16,062 : INFO : EPOCH - 8 : training on 260890 raw words (185371 effective words) took 2.8s, 66708 effective words/s\n",
      "2019-11-26 16:28:17,297 : INFO : EPOCH 9 - PROGRESS: at 42.09% words, 67563 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:28:18,347 : INFO : EPOCH 9 - PROGRESS: at 80.36% words, 67588 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:28:18,828 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 16:28:18,841 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 16:28:18,845 : INFO : EPOCH - 9 : training on 260890 raw words (185543 effective words) took 2.7s, 68729 effective words/s\n",
      "2019-11-26 16:28:19,888 : INFO : EPOCH 10 - PROGRESS: at 34.43% words, 61414 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:28:20,911 : INFO : EPOCH 10 - PROGRESS: at 72.71% words, 65456 words/s, in_qsize 3, out_qsize 0\n",
      "2019-11-26 16:28:21,585 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 16:28:21,638 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 16:28:21,641 : INFO : EPOCH - 10 : training on 260890 raw words (185378 effective words) took 2.8s, 66523 effective words/s\n",
      "2019-11-26 16:28:21,642 : INFO : training on a 2608900 raw words (1854259 effective words) took 28.4s, 65225 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1854259, 2608900)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save to file, can be useful later**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-26 16:28:21,662 : INFO : saving Word2Vec object under word2vecGadamer.w2v, separately None\n",
      "2019-11-26 16:28:21,666 : INFO : not storing attribute vectors_norm\n",
      "2019-11-26 16:28:21,670 : INFO : not storing attribute cum_table\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-11-26 16:28:21,856 : INFO : saved word2vecGadamer.w2v\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
